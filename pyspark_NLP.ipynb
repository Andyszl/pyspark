{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建SparkSession对象\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame([(1,'I really liked this movie'),\n",
    "                         (2,'I would recommend this movie to my friends'),\n",
    "                         (3,'movie was alright but acting was horrible'),\n",
    "                         (4,'I am never watching that movie ever again')],\n",
    "                        ['user_id','review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------+\n",
      "|user_id|review                                    |\n",
      "+-------+------------------------------------------+\n",
      "|1      |I really liked this movie                 |\n",
      "|2      |I would recommend this movie to my friends|\n",
      "|3      |movie was alright but acting was horrible |\n",
      "|4      |I am never watching that movie ever again |\n",
      "+-------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization=Tokenizer(inputCol='review',outputCol='tokens')\n",
    "tokenized_df=tokenization.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "|user_id|review                                    |tokens                                             |\n",
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "|1      |I really liked this movie                 |[i, really, liked, this, movie]                    |\n",
      "|2      |I would recommend this movie to my friends|[i, would, recommend, this, movie, to, my, friends]|\n",
      "|3      |movie was alright but acting was horrible |[movie, was, alright, but, acting, was, horrible]  |\n",
      "|4      |I am never watching that movie ever again |[i, am, never, watching, that, movie, ever, again] |\n",
      "+-------+------------------------------------------+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_df.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df=stopword_removal.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "|user_id|tokens                                             |refined_tokens                    |\n",
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "|1      |[i, really, liked, this, movie]                    |[really, liked, movie]            |\n",
      "|2      |[i, would, recommend, this, movie, to, my, friends]|[recommend, movie, friends]       |\n",
      "|3      |[movie, was, alright, but, acting, was, horrible]  |[movie, alright, acting, horrible]|\n",
      "|4      |[i, am, never, watching, that, movie, ever, again] |[never, watching, movie, ever]    |\n",
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df.select(['user_id','tokens','refined_tokens']).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计数向量器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')\n",
    "#cv_df=count_vec.fit(refined_df).transform(refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df=count_vec.fit(refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'horrible',\n",
       " 'liked',\n",
       " 'alright',\n",
       " 'friends',\n",
       " 'recommend',\n",
       " 'acting',\n",
       " 'never',\n",
       " 'really',\n",
       " 'watching',\n",
       " 'ever']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+---------------------------------+\n",
      "|user_id|refined_tokens                    |features                         |\n",
      "+-------+----------------------------------+---------------------------------+\n",
      "|1      |[really, liked, movie]            |(11,[0,2,8],[1.0,1.0,1.0])       |\n",
      "|2      |[recommend, movie, friends]       |(11,[0,4,5],[1.0,1.0,1.0])       |\n",
      "|3      |[movie, alright, acting, horrible]|(11,[0,1,3,6],[1.0,1.0,1.0,1.0]) |\n",
      "|4      |[never, watching, movie, ever]    |(11,[0,7,9,10],[1.0,1.0,1.0,1.0])|\n",
      "+-------+----------------------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_df.transform(refined_df).select(['user_id','refined_tokens','features']).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF,IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_vec=HashingTF(inputCol='refined_tokens',outputCol='tf_features', numFeatures=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_df=hashing_vec.transform(refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+-------------------------------------+\n",
      "|user_id|refined_tokens                    |tf_features                          |\n",
      "+-------+----------------------------------+-------------------------------------+\n",
      "|1      |[really, liked, movie]            |(100,[12,39,88],[1.0,1.0,1.0])       |\n",
      "|2      |[recommend, movie, friends]       |(100,[16,39,99],[1.0,1.0,1.0])       |\n",
      "|3      |[movie, alright, acting, horrible]|(100,[5,23,39,66],[1.0,1.0,1.0,1.0]) |\n",
      "|4      |[never, watching, movie, ever]    |(100,[39,75,81,94],[1.0,1.0,1.0,1.0])|\n",
      "+-------+----------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashing_df.select(['user_id','refined_tokens','tf_features']).show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec=IDF(inputCol='tf_features',outputCol='tf_idf_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df=tf_idf_vec.fit(hashing_df).transform(hashing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------------------------------------------------+\n",
      "|user_id|tf_idf_features                                                                   |\n",
      "+-------+----------------------------------------------------------------------------------+\n",
      "|1      |(100,[12,39,88],[0.9162907318741551,0.0,0.9162907318741551])                      |\n",
      "|2      |(100,[16,39,99],[0.9162907318741551,0.0,0.9162907318741551])                      |\n",
      "|3      |(100,[5,23,39,66],[0.9162907318741551,0.9162907318741551,0.0,0.9162907318741551]) |\n",
      "|4      |(100,[39,75,81,94],[0.0,0.9162907318741551,0.9162907318741551,0.9162907318741551])|\n",
      "+-------+----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_df.select(['user_id','tf_idf_features']).show(4,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用机器学习进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df=spark.read.csv('Movie_reviews.csv',inferSchema=True,header=True,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7087 2\n"
     ]
    }
   ],
   "source": [
    "print(text_df.count(),len(text_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+---------+\n",
      "|Review                                                                  |Sentiment|\n",
      "+------------------------------------------------------------------------+---------+\n",
      "|I like Mission Impossible movies because you never know who's on the rig|1        |\n",
      "|Brokeback Mountain is a beautiful movie...                              |1        |\n",
      "|I love Brokeback Mountain.                                              |1        |\n",
      "|we're gonna like watch Mission Impossible or Hoot.(                     |1        |\n",
      "|Now, I am the first person to say that The Da Vinci Code sucks, but hell|0        |\n",
      "|the da vinci code is awesome!                                           |1        |\n",
      "|loved the preview for mission impossible III.                           |1        |\n",
      "|Then snuck into Brokeback Mountain, which is the most depressing movie I|0        |\n",
      "|Always knows what I want, not guy crazy, hates Harry Potter..           |0        |\n",
      "|I hate Harry Potter, it's retarted, gay and stupid and there's only one |0        |\n",
      "+------------------------------------------------------------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.orderBy(rand()).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           Sentiment|count|\n",
      "+--------------------+-----+\n",
      "|                  ,0|    1|\n",
      "|. but \"\" Angel an...|    1|\n",
      "|                   0| 3081|\n",
      "| \"\" you see Demen...|    1|\n",
      "| but due to the s...|    1|\n",
      "| the story of \"\" ...|    1|\n",
      "| and not because ...|    1|\n",
      "|            oddly e\"|    1|\n",
      "|   but I still feel\"|    1|\n",
      "|              my God|    1|\n",
      "| I decided to wri...|    1|\n",
      "| but it was reall...|    1|\n",
      "|  but I hate the Da\"|    1|\n",
      "|                   1| 3909|\n",
      "| but immensely we...|    1|\n",
      "|             with f\"|    1|\n",
      "|               also\"|   80|\n",
      "|      or how I love\"|    1|\n",
      "|                 Joe|    1|\n",
      "| which was really...|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.groupBy(\"Sentiment\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df=text_df.filter(((text_df.Sentiment =='1') | (text_df.Sentiment =='0')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6990, 2)\n"
     ]
    }
   ],
   "source": [
    "print((text_df.count(),len(text_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = text_df.withColumn(\"Label\", text_df.Sentiment.cast('float')).drop('Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+-----+\n",
      "|Review                                                                  |Label|\n",
      "+------------------------------------------------------------------------+-----+\n",
      "|i heard da vinci code sucked soo much only 2.5 stars:                   |0.0  |\n",
      "|friday hung out with kelsie and we went and saw The Da Vinci Code SUCKED|0.0  |\n",
      "|I hate it though, because I really like his Mission Impossible films, so|1.0  |\n",
      "|And they all involved Harry Potter * is lame *..                        |0.0  |\n",
      "|Brokeback Mountain was boring.                                          |0.0  |\n",
      "|I hate Harry Potter.                                                    |0.0  |\n",
      "|The Da Vinci Code was absolutely AWESOME!                               |1.0  |\n",
      "|i just love Da Vinci Code so much!                                      |1.0  |\n",
      "|Brokeback Mountain was boring.                                          |0.0  |\n",
      "|As I sit here, watching the MTV Movie Awards, I am reminded of how much |0.0  |\n",
      "+------------------------------------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.orderBy(rand()).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据标记并去除通用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization=Tokenizer(inputCol='Review',outputCol='tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df=tokenization.transform(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|              Review|Label|              tokens|\n",
      "+--------------------+-----+--------------------+\n",
      "|The Da Vinci Code...|  1.0|[the, da, vinci, ...|\n",
      "|this was the firs...|  1.0|[this, was, the, ...|\n",
      "|i liked the Da Vi...|  1.0|[i, liked, the, d...|\n",
      "|i liked the Da Vi...|  1.0|[i, liked, the, d...|\n",
      "|I liked the Da Vi...|  1.0|[i, liked, the, d...|\n",
      "|that's not even a...|  1.0|[that's, not, eve...|\n",
      "|I loved the Da Vi...|  1.0|[i, loved, the, d...|\n",
      "|i thought da vinc...|  1.0|[i, thought, da, ...|\n",
      "|The Da Vinci Code...|  1.0|[the, da, vinci, ...|\n",
      "|I thought the Da ...|  1.0|[i, thought, the,...|\n",
      "|The Da Vinci Code...|  1.0|[the, da, vinci, ...|\n",
      "|The Da Vinci Code...|  1.0|[the, da, vinci, ...|\n",
      "|then I turn on th...|  1.0|[then, i, turn, o...|\n",
      "|The Da Vinci Code...|  1.0|[the, da, vinci, ...|\n",
      "|i love da vinci c...|  1.0|[i, love, da, vin...|\n",
      "|i loved da vinci ...|  1.0|[i, loved, da, vi...|\n",
      "|TO NIGHT:: THE DA...|  1.0|[to, night::, the...|\n",
      "|THE DA VINCI CODE...|  1.0|[the, da, vinci, ...|\n",
      "|Thing is, I enjoy...|  1.0|[thing, is,, i, e...|\n",
      "|very da vinci cod...|  1.0|[very, da, vinci,...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_text_df=stopword_removal.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|              Review|Label|              tokens|      refined_tokens|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|The Da Vinci Code...|  1.0|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|this was the firs...|  1.0|[this, was, the, ...|[first, clive, cu...|\n",
      "|i liked the Da Vi...|  1.0|[i, liked, the, d...|[liked, da, vinci...|\n",
      "|i liked the Da Vi...|  1.0|[i, liked, the, d...|[liked, da, vinci...|\n",
      "|I liked the Da Vi...|  1.0|[i, liked, the, d...|[liked, da, vinci...|\n",
      "|that's not even a...|  1.0|[that's, not, eve...|[even, exaggerati...|\n",
      "|I loved the Da Vi...|  1.0|[i, loved, the, d...|[loved, da, vinci...|\n",
      "|i thought da vinc...|  1.0|[i, thought, da, ...|[thought, da, vin...|\n",
      "|The Da Vinci Code...|  1.0|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|I thought the Da ...|  1.0|[i, thought, the,...|[thought, da, vin...|\n",
      "|The Da Vinci Code...|  1.0|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|The Da Vinci Code...|  1.0|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|then I turn on th...|  1.0|[then, i, turn, o...|[turn, light, rad...|\n",
      "|The Da Vinci Code...|  1.0|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|i love da vinci c...|  1.0|[i, love, da, vin...|[love, da, vinci,...|\n",
      "|i loved da vinci ...|  1.0|[i, loved, da, vi...|[loved, da, vinci...|\n",
      "|TO NIGHT:: THE DA...|  1.0|[to, night::, the...|[night::, da, vin...|\n",
      "|THE DA VINCI CODE...|  1.0|[the, da, vinci, ...|[da, vinci, code,...|\n",
      "|Thing is, I enjoy...|  1.0|[thing, is,, i, e...|[thing, is,, enjo...|\n",
      "|very da vinci cod...|  1.0|[very, da, vinci,...|[da, vinci, code,...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_text_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计数向量器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')\n",
    "cv_text_df=count_vec.fit(refined_text_df).transform(refined_text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|      refined_tokens|            features|Label|\n",
      "+--------------------+--------------------+-----+\n",
      "|[da, vinci, code,...|(2302,[0,1,4,43,2...|  1.0|\n",
      "|[first, clive, cu...|(2302,[11,51,229,...|  1.0|\n",
      "|[liked, da, vinci...|(2302,[0,1,4,53,3...|  1.0|\n",
      "|[liked, da, vinci...|(2302,[0,1,4,53,3...|  1.0|\n",
      "|[liked, da, vinci...|(2302,[0,1,4,53,6...|  1.0|\n",
      "|[even, exaggerati...|(2302,[46,229,271...|  1.0|\n",
      "|[loved, da, vinci...|(2302,[0,1,22,30,...|  1.0|\n",
      "|[thought, da, vin...|(2302,[0,1,4,228,...|  1.0|\n",
      "|[da, vinci, code,...|(2302,[0,1,4,33,2...|  1.0|\n",
      "|[thought, da, vin...|(2302,[0,1,4,223,...|  1.0|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_text_df.select(['refined_tokens','features','Label']).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建模数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text_df=cv_text_df.select(['features','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembler = VectorAssembler(inputCols=['features'],outputCol='features_vec')\n",
    "model_text_df = df_assembler.transform(model_text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      " |-- features_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_text_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df,test_df=model_text_df.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg=LogisticRegression(featuresCol='features_vec',labelCol='Label').fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=log_reg.evaluate(test_df).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=MulticlassClassificationEvaluator(labelCol='Label',metricName='accuracy').evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9775219298245614"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
